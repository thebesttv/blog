#+title: Hugging Face

* Tokenizer

#+begin_src python
  from transformers import AutoTokenizer

  checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
  tokenizer = AutoTokenizer.from_pretrained(checkpoint)

  raw_inputs = [
      "I've been waiting for a HuggingFace course my whole life.",
      "I hate this so much!",
  ]
  inputs = tokenizer(raw_inputs,
                     padding=True, truncation=True, return_tensors="pt")
  print(inputs)
  # {
  #     'input_ids': tensor([
  #         [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
  #         [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
  #     ]),
  #     'attention_mask': tensor([
  #         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
  #         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
  #     ])
  # }
#+end_src

* Model

** AutoModel

#+begin_src python
  from transformers import AutoModel

  checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
  model = AutoModel.from_pretrained(checkpoint)

  outputs = model(**inputs)
  print(outputs.last_hidden_state.shape)
  # torch.Size([2, 16, 768])
#+end_src

This architecture contains only the base Transformer module: given
some inputs, it outputs what we’ll call /hidden states/, also known as
features.  For each model input, we’ll retrieve a high-dimensional
vector representing the *contextual understanding of that input by the
Transformer model*.

While these hidden states can be useful on their own, they’re usually
inputs to another part of the model, known as the *head*.

The vector output by the Transformer module is usually large.  It
generally has three dimensions:
- *Batch size*: The number of sequences processed at a time (2 in our
  example).
- *Sequence length*: The length of the numerical representation of the
  sequence (16 in our example).
- *Hidden size*: The vector dimension of each model input.

** AutoModelForSequenceClassification

#+begin_src python
  from transformers import AutoModelForSequenceClassification

  checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
  model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
  outputs = model(**inputs)

  print(outputs.logits.shape)
  # torch.Size([2, 2])

  print(outputs.logits)
  # tensor([[-1.5607,  1.6123],
  #         [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)

  import torch

  predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
  print(predictions)
  # tensor([[4.0195e-02, 9.5980e-01],
  #         [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)

  model.config.id2label
  # {0: 'NEGATIVE', 1: 'POSITIVE'}
#+end_src
