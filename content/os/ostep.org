#+title: A not-so-short summary on each chapter of OSTEP

# disable numbering of heading
#+OPTIONS: num:nil

-----

- [[https://pages.cs.wisc.edu/~remzi/OSTEP/][OSTEP website]]

* 6. Mechanism: Limited Direct Execution

- TODO [[https://stackoverflow.com/q/12911841/11938767][kernel stack and user space stack]]

The basic idea of limited direct execution is simply to set up the
hardware in advance (during boot in kernel mode, set up trap table &
timer) so as to limit what the process can to without OS assistance.
It requires two key support from the hardware: privilege levels and
timer interrupts.  The former isolating normal process from performing
malicious operations, the latter ensuring the OS can be in control
even when a process enters an infinite loop.

On boot, the OS needs to initialize the trap table and start the
interrupt timer.  The trap table stores both a syscall handler and a
timer handler so that every time the program makes a syscall or the
timer interrupt arrives, the hardware knows what to do.

*Kernel stack* is used to save and restore process registers (general
& PC) when trapping into kernel mode and returning-from-trap to user
mode.

During a timer interrupt & context switch, there are two types of
register saves (or restores).  For example, process A is interrupted
by hardware timer, and OS switches to process B.

The first save happens when the timer interrupt occurs: the hardware
saves A's *user regs* to A's kernel stack (this happens every time
trap executes, with our without context switch, because hardware will
later jump to a trap handler (kernel mode function)), then goes to
kernel mode and jumps to trap handler.  OS now decides to run a
context switch (to B) and here comes the second reg save (specific to
context switch): the OS software (kernel mode trap handler) saves
current regs (*kernel regs* when trapping from A, this includes A's
kernel stack pointer, as later B's kernel stack pointer will replace
it) to A's memory.  The first save is performed by hardware on user
regs, while the second by software on kernel regs.

Now the OS is ready to switch to B.  There will be two restores
happening in reverse order of the saves.  First, the OS restores B's
*kernel regs* from B's memory (this involves restoring B's kernel
stack pointer, so kernel stack is switched from A's to B's).  Now it's
as if the OS just trapped from B.  Then the OS returns-from-trap to
B's user mode code.  The hardware will perform the second restore,
restoring B's *user regs* from B's kernel stack.

Note that the second save (A's kernel reg store) and the first restore
(B's kernel regs restore) happen in the same context switch function.
An example is the xv6 context switch code, in =kernel/swtch.S=.

#+begin_details xv6 context switch
#+begin_src asm
  # Context switch
  #
  #   void swtch(struct context *old, struct context *new);
  #
  # Save current registers in old. Load from new.

  .globl swtch
  swtch:
          sd ra, 0(a0)
          sd sp, 8(a0)
          sd s0, 16(a0)
          sd s1, 24(a0)
          sd s2, 32(a0)
          sd s3, 40(a0)
          sd s4, 48(a0)
          sd s5, 56(a0)
          sd s6, 64(a0)
          sd s7, 72(a0)
          sd s8, 80(a0)
          sd s9, 88(a0)
          sd s10, 96(a0)
          sd s11, 104(a0)

          ld ra, 0(a1)
          ld sp, 8(a1)
          ld s0, 16(a1)
          ld s1, 24(a1)
          ld s2, 32(a1)
          ld s3, 40(a1)
          ld s4, 48(a1)
          ld s5, 56(a1)
          ld s6, 64(a1)
          ld s7, 72(a1)
          ld s8, 80(a1)
          ld s9, 88(a1)
          ld s10, 96(a1)
          ld s11, 104(a1)

          ret
#+end_src
#+end_details

* 7. Scheduling: Introduction

- TODO [[https://en.wikipedia.org/wiki/Scheduling_(computing)#Process_scheduler][Process scheduler -- Wiki]] about long-/medium-/short-term scheduling

Introduces very simple scheduling policies.

Scheduling metrics:
- performance
  - turnaround time: time between arrival to completion,
    T_{completion} - T_{arrival}
- fairness
  - response time: time between arrival and first time execution,
    T_{first run} - T_{arrival}

A non-preemptive scheduler would run each job to completion before
considering whether to run a new job, while a preemptive scheduler
would often stop one process (either because another more urgent job
arrived, or a timer interrupt occurs) in order to run another.

#+begin_details Fairness vs Performance
Performance and fairness are often at odds in scheduling; a scheduler,
for example, may optimize performance but at the cost of preventing a
few jobs from running, thus decreasing fairness.

More generally, any policy (such as RR) that is fair, i.e., that
evenly divides the CPU among active processes on a small time scale,
will perform poorly on metrics such as turnaround time.  Indeed, this
is an inherent trade-off: if you are willing to be unfair, you can run
shorter jobs to completion, but at the cost of response time; if you
instead value fairness, response time is lowered, but at the cost of
turnaround time.  This type of trade-off is common in systems; you
can’t have your cake and eat it too.
#+end_details

** First In, First Out (FIFO) / First Come, First Served (FCFS)

Non-preemptive.  Subject to [[https://dl.acm.org/doi/10.1145/850657.850659][the convoy effect]], where a number of short
jobs get queued behind a very long job.

** Shortest Job First (SJF)

Non-preemptive.  If a long-running process arrives just a hair before
many short-running processes, the latter ones have to wait.

** Shortest Time-to-Completion First (STCF) / Preemptive SJF

Preemptive.  Reschedules every time a new process arrives (or one
completes).  The one with the least remaining time runs.

** Round Robin (RR)

Good for response time, but bad for turnaround time.  Cycle through
the jobs, running each one for a specific time slice.  Reducing time
slice reduces response time, but the overhead of a context switch will
slowly dominate the whole time slice.

#+begin_details
Thus, deciding on the length of the time slice presents a trade-off to
a system designer, making it long enough to *amortize* the cost of
switching without making it so long that the system is no longer
responsive.

Note that the cost of context switching does not arise solely from the
OS actions of saving and restoring a few registers.  When programs
run, they build up a great deal of state in CPU caches, TLBs, branch
predictors, and other on-chip hardware.  Switching to another job
causes this state to be flushed and new state relevant to the
currently-running job to be brought in, which may exact a noticeable
performance cost.
#+end_details

* 8. Scheduling: The Multi-Level Feedback Queue

MLFQ has /multiple levels/ of /queues/, and uses /feedback/ to
determine the priority of a given job.  It tries to minimize /response
time/ for interactive jobs while also minimizing /turnaround time/
without /a priori/ knowledge of job length.  It learns about processes
as they run, and thus use the history of the job to predict its future
behavior.

#+begin_details MLFQ approximates SJF
because it doesn’t /know/ whether a job will be a short job or a
long-running job, it first /assumes/ it might be a short job, thus
giving the job high priority.  If it actually is a short job, it will
run quickly and complete; if it is not a short job, it will slowly
move down the queues, and thus soon prove itself to be a long-running
more batch-like process.  In this manner, MLFQ approximates SJF.
#+end_details

The MLFQ has a number of distinct queues, each assigned a different
priority level.  Rules:
1. If Priority(A) > Priority(B), A runs (B doesn't).
2. If Priority(A) = Priority(B), A & B run in round-robin fashion
   using the time slice (quantum length) of the given queue.
3. When a job enters the system, it is placed at the highest priority
   (the topmost queue).
4. *Time Allotment*: Once a job uses up its time allotment at a given
   level (regardless of how many times it has given up the CPU), its
   priority is reduced (i.e., it moves down one queue).
5. *Priority Boost*: After some time period S, move all the jobs in
   the system to the topmost queue.

* 9. Scheduling: Proportional Share

The proportional-share scheduler, or fair-share scheduler, tries to
guarantee that each job obtains *a certain percentage* of CPU time.

** Lottery Scheduling

One such example is *lottery scheduling*, using a very basic idea:
*tickets*, which represent the share of the resource that a consumer
(process) should receive.  Lottery scheduling provide some mechanisms
to manipulate tickets:
- *ticket currency*: in a system with a total of 100 tickets, process
  A has 50 tickets and two threads (A_1, A_2), B has 50 tickets and
  only one thread (B_1).  A gives A_1 and A_2 both 1 ticket (under a
  different currency), B gives B_1 100 tickets (also a different
  currency).  In effect, out of the total 100 tickets, A_1 has 25, A_2
  has 25, B_1 has 50.
- *ticket transfer*: when a client requires a server to run a task,
  the client can hand off its tickets to the server, essentially
  speeding up server.
- *ticket inflation*: in a *trusted* group of processes, if one of
  them needs more CPU (or less CPU), it can temporarily increase (or
  decrease) its tickets.

However, the problem of how to properly assign tickets to processes
remains open.

** Stride scheduling

Stride scheduling is a *deterministic* (as opposed to the random
lottery scheduling) fair-share scheduler.  It uses *stride* instead
(stride is simply the inverse of ticket), and maintains a variable
=pass= for each process.  Where lottery scheduling achieves the
proportions probabilistically over time; stride scheduling gets them
exactly right at the end of each scheduling cycle.  However, with the
introduction of a global state per process (=pass=), when a new
process arrives, setting an appropriate value of its new =pass= is a
new problem.

** The Linux Completely Fair Scheduler (CFS)

Since there can be many processes, CFS uses a red-black tree to
maintain the current ready process list.

* TODO 10. Multiprocessor Scheduling (Advanced)

* 13. The Abstraction: Address Spaces

The *address space* is the abstraction OS provides to the running program; it
is the running program's view of memory in the system.  It contains all of the
memory state of process, including code, stack, heap (and other data such as
statically allocated global variable).

Goals of a virtual memory (VM):
- transparency: the illusion provided by the OS shouldn't be visible to
  applications
- efficiency: both in time & in space.  Time efficiency requires hardware
  support such as TLB
- protection & isolation: protect processes from one another, as well as the OS
  itself form processes

* 14. Interlude: Memory API

- =brk=
- =mmap=

* 15. Mechanism: Address Translation

#+begin_quote
Through virtualization, the OS (with the hardware's help) turns the ugly
machine reality into a useful, powerful, and easy to use abstraction.
#+end_quote

Hardware-based address translation extends the idea of limited direct execution
to virtual memory---the OS just sets up the hardware appropriately and lets the
process run directly on CPU (efficiency); only when the process misbehaves does
the OS have to become involved (control).  This way both efficiency and control
is achieved.

A simple way of memory virtualization is *base-and-bounds*, where the OS
maintains a pair of base & bounds values for each process, limiting their
locations in physical memory.  *Base* indicates the start of the address space
and *bounds* the size (or end) of it.  The hardware maintains a pair of base &
bounds registers for *each CPU* constraining the address space of _the current
running process_.  On each memory operation (e.g., instruction fetch and
load/store instruction), the hardware adds the base register to the virtual
address and checks if the resulting /effective (physical) address/ is within
bounds before performing memory IO.  If the address exceeds the process's
address space, the hardware raises an "out-of-bounds" exception.  Because this
relocation of the address happens at runtime, and because the OS can move the
address space even after the process has started running, this technique is
often called *dynamic relocation*.

This technique of dynamic relocation, although simple, can cause *internal
fragmentation*, where a process does not use most of its space between heap and
stack and no other process can utilize this part of memory, thus much of the
memory space is wasted.

* 16. Segmentation

The problem of internal fragmentation raises the need for a more
flexible mechanism called *segmentation* that supports a large address
space with (potentially) much free space between the stack & heap.

The different growing directions of stack & heap requires the MMU to
add one bit to each segment so as to keep track of in which direction
it grows.

Adding hardware *protection bits* the control whether a process can
read, write, or execute a particular segment enables *code sharing*,
where one read-only (and executable) segment can be shared by multiple
processes running the same program.

However, as processes are created, the available physical memory space
can still be divided into small chunks, a problem called *external
fragmentation*.  [[https://stackoverflow.com/a/24980816/11938767][Here]]'s an excellent clarification of external vs
internal fragmentation.

Allocating variable-sized segments in memory leads to many problems.
The only solution is to never allocate memory in variable-sized
chunks.  Segmentation still isn't flexible enough.

* 17. Free-Space Management

Managing variable-sized memory units can be a challenging task.  Many
solutions are proposed that deal with external fragmentation.

** Mechanisms

There are three common mechanisms used in most allocators:
- splitting & coalescing (merging): splitting a free unit into two
  smaller chunks and merging two neighboring free units into a big
  one.
- tracking the size of allocated regions: =free(void *ptr)= does not
  take a size argument; thus it is assumed that given a pointer, the
  malloc library can quickly determine the size of the region of
  memory being freed and thus incorporate the space back into the free
  list.  Normally, a header structure (=header_t=) is placed just
  before the address returned by =malloc=.  So the total allocated
  space is user requested size plus size of the header structure.
  #+begin_src c
    typedef struct {
      int size;
      int magic;
    } header_t;
  #+end_src
- embedding a free list: Managing the free list requires building a
  free list inside the free space itself.  Another header structure
  (=node_t=) is needed for each free space.  It is also immediately
  before the free space, denoting its size and the next free node in
  the list.
  #+begin_src c
    typedef struct __node_t {
      int              size;
      struct __node_t *next;
    } node_t;
  #+end_src
  =malloc= returns the address of the allocated space *after*
  =header_t=, while the =next= field of =node_t= points to the
  =node_t= of the next free portion.  The address of =node_t= starts a
  documented free portion, not the address after it.
- growing the heap

** Basic Strategies

When a request for allocating a particular size of memory arrives,
there are a few basic strategies:
- best fit: traverse the free list, return the smallest region
  satisfying the request.  This can result in many very small
  fragmentations.  Also, traversing the free list is a very expensive
  operation.
- worst fit: return the largest region satisfying the request.  This
  approach tries to minimize the very small fragmentations produced by
  /best fit/.  However, this performs badly, leading to excess
  fragmentation while still having high overheads.
- first fit: simply return the first blog that is big enough.
- next fit: instead of searching starting from the head of the list
  like /first fit/, /next fit/ remembers the entry last allocated, and
  starts searching after that.  The idea is to spread the searches
  throughout the list more uniformly.

** Other Approaches

*** Segregated Lists

Segregated lists tries to *make the common case fast* by observing
whether a particular application has one (or a few) *popular-sized*
request that it makes, and maintaining a separate list just to manage
objects of that size.

*** Buddy Allocation

Binary buddy allocator tries to ease the merging of two free spaces.
The size of total heap is $2^N$.  The size of a free space within the
heap is always $2^n$ ($n < N$), forming a tree-like structure.  During
an malloc request, the allocator starts with $2^N$, and recurses down
the tree to the smallest chuck of size $2^m$.  When that portion is
freed, the allocator checks whether its sibling (aka. its buddy) is
also free, if it is, then merges the two units.  Note that this
approach suffers from internal fragmentation, as you are only allowed
to give out power-of-two-sized blocks.

* 18. Paging: Introduction

Instead of splitting up a process's address space into some number of
variable-sized logical segments (e.g., code, heap, stack), we divide
it into *fixed-sized units*, each of which we call a *page*.  The
physical memory is viewed as an array of fixed-sized slots called
*page frames*; each of the frames can contain a single virtual-memory
page.  Correspondingly, we view physical memory as an array of
fixed-sized slots called *page frames*; each of these frames can
contain a single virtual-memory page.

The OS keeps a page table for *each* running process, since each one
has a separate address space.  In a page table, each *pate table entry
(PTE)* stores a mapping between a *virtual page number (VPN)* to a
*physical frame number (PFN)*.  On a memory access, the virtual
address is split into a VPN and an offset.  The OS first consults the
*page table entry* (indexed by the VPN) for the PFN of the requested
address, then concatenates PFN with the offset, producing the actual
physical address.

The raw form of paging has two problems:
- too large: page table can be very large: on a 32-bit physical memory
  (4GB) with a 4KB page table, a single page table can have as many as
  $2^{32} / 2^{12} = 2^{20}$ PTEs.  If each PTE takes up 4B, then that
  would be a total of 4MB per process just to store its page table.
  If there are 100 processes, then 400MB will be needed!
- too slow: each memory access needs to consult the page table, which
  is another memory access overhead, making the process slower by at
  least a factor of two

* 19. Paging: Faster Translations (TLBs)

A translation-lookaside buffer (TLB) is part of the chip's
memory-management unit (MMU), and is simply a *hardware cache* of
popular virtual-to-physical address translations; thus, a better name
would be an address-translation cache.

On each memory access, the hardware first checks if the TLB holds the
entry for the VPN of the virtual address.  Just like a regular cache
has hits & misses, there can be a TLB hit or a TLB miss.  On a TLB
hit, the TLB simply calculates the physical address and accesses the
memory.  On a TLB miss, however, the TLB will have to consult the
original (full) page table in memory for the PTE, store it in TLB, and
retry the access operation.

** Who Handles the TLB Miss?

*** Hardware

In the old days of CISC computers, hardware handles TLB misses
entirely.  To do this, the hardware needs to know everything about the
page table: where they are stored in memory and the exact data
structure.

*** Software

On RISC machines, the hardware simply raises an exception on a TLB
miss and let the OS handles it.  When the OS is done updating the TLB,
the hardware retries the access.

This raises a few issues:
- different from the return-from-trap instruction in other exception
  handlers that resumes execution *after* the instruction that raised
  the exception, the return-from-trap in a TLB miss needs to resume
  execution *still at* the memory access instruction that caused the
  miss, not after it, so the memory access is retried.
- when handling a TLB miss, the OS needs to be careful not to raise an
  infinite chain of TLB misses to occur.

** On Context Switch

Since each process has a different page table, the TLB entries for the
last process are meaningless to the about-to-be-run process.  Some
hardware systems provide an address space identifier (ASID) field in
each TLB entry to distinguish between the address spaces of different
processes.  An ASID is just like a process identifier (PID) except it
has fewer bits.  With ASIDs, the TLB can hold translations from
different processes at the same time without confusion.

* 20. Paging: Smaller Tables

Simple array-based page tables (aka. linear page tables) are too big.
Several solutions are proposed in order to make page tables smaller.
Using bigger page sizes can cause internal fragmentation; as for the
hybrid approach of having one page table *per segment* (e.g., code,
heap, stack) for each process, it now introduces variable-sized page
tables, which can be hard to manage and will again cause external
fragmentation to arise.

** Multi-level Page Tables

This approach does not use segmentation, instead it turns the page
table into a tree-like structure.  The page table is divided into
page-sized units, each one being.  An extra *page directory* is used
to track where each page-sized unit is in memory (or whether the unit
is not allocated because all the PTEs therein are not valid).

The page directory, in a simple two-level table, consists of a number
of *page directory entries* (PDE).  A PDE (minimally) has a *valid
bit* and a *page frame number* (PFN), similar to a PTE.  If the PDE is
valid, it means that *at least one* of the pages of the page table
that the entry points to (via the PFN) is valid, i.e., in at least one
PTE on that page pointed to by this PDE, the valid bit in that PTE is
set to one.  If the PDE is not valid (i.e., equal to zero), the rest
of the PDE is not defined.

The multi-level page table adds a *level of indirection* through the
use of page directory, allowing page table units to reside on
arbitrary location in memory, as opposed to the linear page table that
requires all PTEs reside continuously in physical memory.

The goal of constructing a multi-level page table is to make each
piece of the page table (including the page table directory) fit
within a single page.  However, in a two-level page table, if there
are too many page directory entries, the page table directory may not
fit within a single page.  This requires the page table to have more
than two levels.

* 21. Beyond Physical Memory: Mechanisms

To support a large virtual address space (maybe even bigger than the
physical RAM), the OS needs to be able to *page in* a page from disk
to memory, and *page out* a page from memory to disk.

A portion of space on the disk (called *swap space*) is reserved for
swapping pages out of and into memory.  A *present bit* is needed for
each PTE to indicate whether the page is actually in memory (present).
On each memory reference, after finding the PTE for the VPN of the
virtual address, the present bit is checked.  If the page is not
present, a *page fault* is triggered.  Upon a page fault, the OS is
invoked (via a *page-fault handler*) to move the page back into
memory.

When swapping in a page, if the memory is already full (or close to
full), then the OS will first page out some pages to make room for the
new page.  Since the penalty of a page miss can be very high, the
*page-replacement policy* needs to be cautious which page to swap out.
Also, a *swap daemon* or *page daemon* is normally used to keep some
free portion of memory always in place and to swap out some pages when
available memory gets low.

* 22. Beyond Physical Memory: Policies

** Memory as Cache for Pages

#+begin_quote
Given that main memory holds some subset of all the pages in the
system, it can rightly be viewed as *a cache for virtual memory pages
in the system*.  Thus, our goal in picking a replacement policy for
this cache is to *minimize the number of cache misses*, i.e., to
minimize the number of times that we have to fetch a page from disk.
Alternately, one can view our goal as *maximizing the number of cache
hits*, i.e., the number of times a page that is accessed is found in
memory.
#+end_quote

The cost of disk access is so high in modern systems that even a tiny
miss rate will quickly dominate the overall AMAT (average memory
access time) of running programs.

** The Optimal Replacement Policy

The optimal (though not the most practical) replacement policy is to
always page out the one that will be accessed *furthest in the
future*.  This essentially says that all other pages that are accessed
before that one are more important.

In any study you perform, knowing what the optimal is lets you perform
a better comparison, showing how much improvement is still possible,
and also when you can stop making your policy better, because it is
close enough to the ideal.

#+begin_details Aside: Types Of Cache Misses
In the computer architecture world, architects sometimes find it
useful to characterize misses by type, into one of three categories:
compulsory, capacity, and conflict misses, sometimes called the Three
C’s. A *compulsory miss* (or *cold-start miss*) occurs because the
cache is empty to begin with and this is the first reference to the
item; in contrast, a *capacity miss* occurs because the cache _ran out
of space_ and had to evict an item to bring a new item into the cache.
The third type of miss (a *conflict miss*) arises in hardware because
of _limits on where an item can be placed in a hardware cache_, due to
something known as set-associativity; it does not arise in the OS page
cache because such caches are always *fully-associative*, i.e., there
are no restrictions on where in memory a page can be placed.
#+end_details

** Simple Policies

- *FIFO* simply keeps a queue of page tables, and evict the earliest
  one.
- *Random* evicts page tables at random.


These simple policies are easy to implement, but they don't do well.
More specifically, they do not have a *stack property*, where a better
hit rate is achieved with a larger cache size.

** Using History: LRU

On each eviction, kick out the least-recently used page.  This method,
although performs well in general, has some weird corner cases.
Assuming there are 51 pages being visited in a loop (0, 1, ..., 50, 0,
1, ...) and a memory of 50 pages, then the hit rate will be zero (when
visiting page 50, page 0 is evicted; then page 1 is evicted when page
0 is visited).

Always finding the exact oldest page can be very hard, since on a 4GB
memory with 4KB page size, 1M entries needs to be tracked for the
oldest one.  Some approximation is in order.

When approximating LRU, a *use bit* (or *reference bit*) is maintained
for each page (maybe in the per-process page table, or another array
somewhere in memory/hardware).  The hardware sets that bit to 1 on
each access to that page.  It's the OS's job to clear the bit on some
occasions.

The *clock algorithm* is a good example.  Imagine the pages are
arranged in a circular list and a clock head points to a certain page.
When a replacement has to occur, the clock head checks the use bit of
the pointed-to page: if it's a 0, then good, it's not recently used,
swap that; otherwise it's 1 and the page is recently used, then *clear
that bit*, point to the next page and repeat.  A 0 is found either
because a certain page has a zero use bit, or that the use bit of all
pages are cleared and the head circles back to the first page.

* 23. Complete Virtual Memory Systems

Covers the VM system of two systems: VAX/VMS & Linux.

- [[https://meltdownattack.com/][Meltdown and Spectre]]

* 26. Concurrency: An Introduction

A *multi-threaded* program has more than one point of execution.  Each
thread of a common process share the address space, but has its own
- PC
- registers, so a context-switch is still needed when switching
  between running threads of the same process (but as address space is
  shared, there's no need to switch page table)
- TCB (thread control block) that stores the state of the thread
- stack that tracks the function call hierarchy

Multiple threads executing the same code can result in a *race
condition*.  That piece of code is called a *critical section* and
needs *mutual exclusion* (can be executed only by one thread at any
time).

With a few useful instructions from the hardware, we can build a
general set of *synchronization primitives* that enable _the atomicity
of instructions_ and _sleeping/waking interactions between two
threads_.

* 27. Interlude: Thread API

When mutual exclusion is required, use a lock.  When signaling between
threads, use conditional variables.

* 28. Locks

Although we would sometimes like to execute a series of instructions
atomically, interrupts and multi-processors make it hard.  Putting
*locks* around a critical section ensures that it is executed *as if*
it were a single atomic instruction.

A lock is either *available* (*unlocked*, or *free*) and thus no
thread holds the lock, or *acquired* (*locked*, or *held*) and thus
*exactly one thread* holds the lock and presumable is in a critical
section.  The holding thread is called the owner of the lock.

Basic criteria for locks:
- *mutual exclusion*: can the lock prevent multiple threads from
  entering a critical section?
- *fairness*: does each thread contending for the lock get a fair shot
  at acquiring it once it is free?  Will any thread starve?
- *performance*: what's the time overhead added by using the lock
  (under different situations)?
  1. no contention: only a single thread is running and it grabs &
     releases the lock
  2. uni-core: multiple threads are contending for the lock on a
     single CPU
  3. multi-core: multiple threads on multiple cores are contending for
     the lock


On a single CPU, the simplest way to implement a lock is to turn off
hardware interrupt on grabbing the lock and to turn on interrupt again
on releasing it.  However,
1. changing timer interrupt is a privileged instruction, and the OS
   cannot regain control once interrupt is turned off, meaning the
   program can go into an endless loop, rendering whole system
   unusable.
2. this approach simply does not work on multiple CPUs.
3. turning off interrupt for extended periods of time can lead to some
   interrupts (such as IO interrupt) becoming lost.
4. turning on / off interrupt is inefficient.


To implement a lock that satifies the three criteria, both hardware
and OS support is needed.

** Hardware support

*** test-and-set

Test-and-set returns the old value of a memory location (so you can
use it in a test) while also setting it to the new value

#+begin_src c
  int TestAndSet(int *old_ptr, int new) {
    int old = *old_ptr;           // store old value
    ,*old_ptr = new;               // set to new value
    return old;                   // return old value
  }

  void Lock(lock_t *lock) {
    while (TestAndSet(&lock->flag, 1) == 1)
      ;                           // spin
  }
#+end_src
***  compare-and-swap / compare-and-exchange

Compare-and-wap (CAS) first tests the old value of a memory location.
If it is some expected value, CAS then sets it to the new value;
otherwise, it does nothing.  Either way, the old value is returned for
inspection.

#+begin_src c
  int CompareAndSwap(int *ptr, int expected, int new) {
    int ori = *ptr;               // Store original value.
    if (ori == expected)          // If the same as expected,
      ,*ptr = new;                 //   set to new value.
    return ori;                   // Either way, return original value.
  }

  void Lock(lock_t *lock) {
    while (CompareAndSwap(&lock->flag, 0, 1) == 1)
      ;                           // spin
  }
#+end_src

*** load-linked & store-conditional

Load-linked (or load-reserved) & store conditional (LL/SC or LR/SC):
Instead of using just one instruction, LL/SC uses two.  LL acts just
like a typical load.  But the difference comes with SC, which only
succeeds (and updates the value stored at the address just load-linked
from) *if no intervening store to the address has taken place*.  In
the case of success, SC returns 1 and updates the location with the
new value; otherwise, it returns 0 and the memory location is left
untouched.

#+begin_src c
  int LoadLinked(int *ptr) {
    return *ptr;
  }
  int StoreConditional(int *ptr, int value) {
    if (no update to *ptr since LoadLinked to this address) {
      ,*ptr = value;
      return 1;                   // success
    } else {
      return 0;                   // fail
    }
  }

  void Lock(lock_t *lock) {
    // Not { flag free & grab flag success }
    while( !(LoadLinked(&lock->flag) == 0 &&
             StoreConditional(&lock->flag, 1) == 1) )
      ;                           // spin
  }
#+end_src

#+begin_details RISC-V & LR/SC
See section 8.2 "Load-Reserved/Store-Conditional Instructions" of The
RISC-V Instruction Set Manual, Volume I: Unprivileged ISA Document
(Version 20191213) for rationals behind RISC-V's choice to use LR/SC
over CAS.
#+end_details

*** fetch-and-add

Fetch-and-add increases a value while returning the old value.  This
can be used to build a ticket lock, which uses two variables (=ticket=
and =turn=) to implement a lock.  Every time a thread wants to grab
the lock, the ticket lock hands out *a ticket (=myTurn=) representing
the calling thread's turn* for the lock.  The thread then spin waits
for its turn.  When a thread releases the lock, it sets the lock for
the next turn.  The ticket lock is *fair*---it _ensures progress for
all waiting threads_.

#+begin_src c
  int FetchAndAdd(int *ptr) {
    int old = *ptr;
    ,*ptr = *ptr + 1;
    return old;
  }

  // Ticket Lock
  typedef struct __lock_t {
    int ticket;
    int turn;
  } lock_t;

  void LockInit(lock_t *lock) {
    lock->ticket = 0;
    lock->turn   = 0;
  }

  void Lock(lock_t lock) {
    int myTurn = FetchAndAdd(&lock->ticket); // get one ticket
    while (lock->turn != myTurn)             // spin waiting for my turn
      ;
  }

  void Unlock(lock_t *lock) {
    lock->turn = lock->turn + 1;
  }
#+end_src

** OS support

The simple spin lock requires a preemptive scheduler, especially on a
single CPU, as without preemption, the thread trying to grab the lock
will be stuck in a loop, never relinquishing the CPU.  In terms of the
criteria, a simple spin lock is:
1. correct: with test-and-set or other hardware support, it is correct.
2. not fair: it provides no fairness guarantee; a thread may starve
3. has high overhead on a single CPU; maybe not bad on multiple CPUs

To address the problems, more sophisticated locking technique is
needed.

*** Just yield

The OS primitive =yield()= moves a thread from running state to ready
state.  When a thread cannot grab the lock, it calls =yield()=, giving
up the CPU and letting others run.  By yielding, the calling thread
essentially *deschedules* itself.

Yielding, however, is not the silver bullet.
- It still does not have great performance.  If the owner of the lock
  needs many time slices to finish the critical section, then the
  yielding threads will need to yield many times.  The cost of a
  context switch is still expensive.
- It still does not guarantee fairness.

*** Using a queue

When a thread is trying to grab an acquired lock, record its tid in a
waiting queue and put it to sleep.  When the lock is released, check
the queue and wakes up the first sleeping thread.

Due consideration is needed to prevent data races.
