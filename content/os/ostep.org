#+title: Summaries on each chapter of OSTEP

# disable numbering of heading
#+OPTIONS: num:nil

-----

- [[https://pages.cs.wisc.edu/~remzi/OSTEP/][OSTEP website]]

* 6. Mechanism: Limited Direct Execution

- TODO [[https://stackoverflow.com/q/12911841/11938767][kernel stack and user space stack]]

The basic idea of limited direct execution is simply to set up the
hardware in advance (during boot in kernel mode, set up trap table &
timer) so as to limit what the process can to without OS assistance.
It requires two key support from the hardware: privilege levels and
timer interrupts.  The former isolating normal process from performing
malicious operations, the latter ensuring the OS can be in control
even when a process enters an infinite loop.

On boot, the OS needs to initialize the trap table and start the
interrupt timer.  The trap table stores both a syscall handler and a
timer handler so that every time the program makes a syscall or the
timer interrupt arrives, the hardware knows what to do.

*Kernel stack* is used to save and restore process registers (general
& PC) when trapping into kernel mode and returning-from-trap to user
mode.

During a timer interrupt & context switch, there are two types of
register saves (or restores).  For example, process A is interrupted
by hardware timer, and OS switches to process B.

The first save happens when the timer interrupt occurs: the hardware
saves A's *user regs* to A's kernel stack (this happens every time
trap executes, with our without context switch, because hardware will
later jump to a trap handler (kernel mode function)), then goes to
kernel mode and jumps to trap handler.  OS now decides to run a
context switch (to B) and here comes the second reg save (specific to
context switch): the OS software (kernel mode trap handler) saves
current regs (*kernel regs* when trapping from A, this includes A's
kernel stack pointer, as later B's kernel stack pointer will replace
it) to A's memory.  The first save is performed by hardware on user
regs, while the second by software on kernel regs.

Now the OS is ready to switch to B.  There will be two restores
happening in reverse order of the saves.  First, the OS restores B's
*kernel regs* from B's memory (this involves restoring B's kernel
stack pointer, so kernel stack is switched from A's to B's).  Now it's
as if the OS just trapped from B.  Then the OS returns-from-trap to
B's user mode code.  The hardware will perform the second restore,
restoring B's *user regs* from B's kernel stack.

Note that the second save (A's kernel reg store) and the first restore
(B's kernel regs restore) happen in the same context switch function.
An example is the xv6 context switch code, in =kernel/swtch.S=.

#+begin_details xv6 context switch
#+begin_src asm
  # Context switch
  #
  #   void swtch(struct context *old, struct context *new);
  #
  # Save current registers in old. Load from new.

  .globl swtch
  swtch:
          sd ra, 0(a0)
          sd sp, 8(a0)
          sd s0, 16(a0)
          sd s1, 24(a0)
          sd s2, 32(a0)
          sd s3, 40(a0)
          sd s4, 48(a0)
          sd s5, 56(a0)
          sd s6, 64(a0)
          sd s7, 72(a0)
          sd s8, 80(a0)
          sd s9, 88(a0)
          sd s10, 96(a0)
          sd s11, 104(a0)

          ld ra, 0(a1)
          ld sp, 8(a1)
          ld s0, 16(a1)
          ld s1, 24(a1)
          ld s2, 32(a1)
          ld s3, 40(a1)
          ld s4, 48(a1)
          ld s5, 56(a1)
          ld s6, 64(a1)
          ld s7, 72(a1)
          ld s8, 80(a1)
          ld s9, 88(a1)
          ld s10, 96(a1)
          ld s11, 104(a1)

          ret
#+end_src
#+end_details

* 7. Scheduling: Introduction

- TODO [[https://en.wikipedia.org/wiki/Scheduling_(computing)#Process_scheduler][Process scheduler -- Wiki]] about long-/medium-/short-term scheduling

Introduces very simple scheduling policies.

Scheduling metrics:
- performance
  - turnaround time: time between arrival to completion,
    T_{completion} - T_{arrival}
- fairness
  - response time: time between arrival and first time execution,
    T_{first run} - T_{arrival}

A non-preemptive scheduler would run each job to completion before
considering whether to run a new job, while a preemptive scheduler
would often stop one process (either because another more urgent job
arrived, or a timer interrupt occurs) in order to run another.

#+begin_details Fairness vs Performance
Performance and fairness are often at odds in scheduling; a scheduler,
for example, may optimize performance but at the cost of preventing a
few jobs from running, thus decreasing fairness.

More generally, any policy (such as RR) that is fair, i.e., that
evenly divides the CPU among active processes on a small time scale,
will perform poorly on metrics such as turnaround time.  Indeed, this
is an inherent trade-off: if you are willing to be unfair, you can run
shorter jobs to completion, but at the cost of response time; if you
instead value fairness, response time is lowered, but at the cost of
turnaround time.  This type of trade-off is common in systems; you
can’t have your cake and eat it too.
#+end_details

** First In, First Out (FIFO) / First Come, First Served (FCFS)

Non-preemptive.  Subject to [[https://dl.acm.org/doi/10.1145/850657.850659][the convoy effect]], where a number of short
jobs get queued behind a very long job.

** Shortest Job First (SJF)

Non-preemptive.  If a long-running process arrives just a hair before
many short-running processes, the latter ones have to wait.

** Shortest Time-to-Completion First (STCF) / Preemptive SJF

Preemptive.  Reschedules every time a new process arrives (or one
completes).  The one with the least remaining time runs.

** Round Robin (RR)

Good for response time, but bad for turnaround time.  Cycle through
the jobs, running each one for a specific time slice.  Reducing time
slice reduces response time, but the overhead of a context switch will
slowly dominate the whole time slice.

#+begin_details
Thus, deciding on the length of the time slice presents a trade-off to
a system designer, making it long enough to *amortize* the cost of
switching without making it so long that the system is no longer
responsive.

Note that the cost of context switching does not arise solely from the
OS actions of saving and restoring a few registers.  When programs
run, they build up a great deal of state in CPU caches, TLBs, branch
predictors, and other on-chip hardware.  Switching to another job
causes this state to be flushed and new state relevant to the
currently-running job to be brought in, which may exact a noticeable
performance cost.
#+end_details

* 8. Scheduling: The Multi-Level Feedback Queue

MLFQ has /multiple levels/ of /queues/, and uses /feedback/ to
determine the priority of a given job.  It tries to minimize /response
time/ for interactive jobs while also minimizing /turnaround time/
without /a priori/ knowledge of job length.  It learns about processes
as they run, and thus use the history of the job to predict its future
behavior.

#+begin_details MLFQ approximates SJF
because it doesn’t /know/ whether a job will be a short job or a
long-running job, it first /assumes/ it might be a short job, thus
giving the job high priority.  If it actually is a short job, it will
run quickly and complete; if it is not a short job, it will slowly
move down the queues, and thus soon prove itself to be a long-running
more batch-like process.  In this manner, MLFQ approximates SJF.
#+end_details

The MLFQ has a number of distinct queues, each assigned a different
priority level.  Rules:
1. If Priority(A) > Priority(B), A runs (B doesn't).
2. If Priority(A) = Priority(B), A & B run in round-robin fashion
   using the time slice (quantum length) of the given queue.
3. When a job enters the system, it is placed at the highest priority
   (the topmost queue).
4. *Time Allotment*: Once a job uses up its time allotment at a given
   level (regardless of how many times it has given up the CPU), its
   priority is reduced (i.e., it moves down one queue).
5. *Priority Boost*: After some time period S, move all the jobs in
   the system to the topmost queue.

* 9. Scheduling: Proportional Share

The proportional-share scheduler, or fair-share scheduler, tries to
guarantee that each job obtains *a certain percentage* of CPU time.

** Lottery Scheduling

One such example is *lottery scheduling*, using a very basic idea:
*tickets*, which represent the share of the resource that a consumer
(process) should receive.  Lottery scheduling provide some mechanisms
to manipulate tickets:
- *ticket currency*: in a system with a total of 100 tickets, process
  A has 50 tickets and two threads (A_1, A_2), B has 50 tickets and
  only one thread (B_1).  A gives A_1 and A_2 both 1 ticket (under a
  different currency), B gives B_1 100 tickets (also a different
  currency).  In effect, out of the total 100 tickets, A_1 has 25, A_2
  has 25, B_1 has 50.
- *ticket transfer*: when a client requires a server to run a task,
  the client can hand off its tickets to the server, essentially
  speeding up server.
- *ticket inflation*: in a *trusted* group of processes, if one of
  them needs more CPU (or less CPU), it can temporarily increase (or
  decrease) its tickets.

However, the problem of how to properly assign tickets to processes
remains open.

** Stride scheduling

Stride scheduling is a *deterministic* (as opposed to the random
lottery scheduling) fair-share scheduler.  It uses *stride* instead
(stride is simply the inverse of ticket), and maintains a variable
=pass= for each process.  Where lottery scheduling achieves the
proportions probabilistically over time; stride scheduling gets them
exactly right at the end of each scheduling cycle.  However, with the
introduction of a global state per process (=pass=), when a new
process arrives, setting an appropriate value of its new =pass= is a
new problem.

** The Linux Completely Fair Scheduler (CFS)

Since there can be many processes, CFS uses a red-black tree to
maintain the current ready process list.

* TODO 10. Multiprocessor Scheduling (Advanced)

* 13. The Abstraction: Address Spaces

The *address space* is the abstraction OS provides to the running
program; it is the running program's view of memory in the system.  It
contains all of the memory state of process, including code, stack,
heap (and other data such as statically allocated global variable).

Goals of a virtual memory (VM):
- transparency: the illusion provided by the OS shouldn't be visible
  to applications
- efficiency: this requires hardware support such as TLBs
- protection & isolation: protect processes from one another, as well
  as the OS itself form the processes

* 14. Interlude: Memory API

- =brk=
- =mmap=

* 15. Mechanism: Address Translation

Address translation extends the idea of limited direct execution to
virtual memory---the OS just sets up the hardware appropriately and
lets the process run directly on the CPU; only when the process
misbehaves does the OS have to become involved.

A simple way of memory virtualization is base-and-bounds, or dynamic
relocation, where the OS maintains a pair of base & bounds value for
each process limiting their locations in physical memory, *base* being
the start of the address space and *bounds* being the size (or end) of
it.  The hardware maintains a pair of base & bounds registers *per
CPU* constraining the address space of the current running process.
On each memory operation (e.g. instruction fetch and load/store
instruction), the hardware adds the base register to the virtual
address and checks if the resulting effective (physical) address is
within bounds before performing memory IO.  If the address exceeds the
process's address space, the hardware raises an "out-of-bounds"
exception.

This technique of dynamic relocation, although simple, can cause
*internal fragmentation*, where a process does not use most of its
space between heap and stack and no other process can utilize this
part of memory, thus much of the memory space is wasted.

* 16. Segmentation

The problem of internal fragmentation raises the need for a more
flexible mechanism called *segmentation* that supports a large address
space with (potentially) much free space between the stack & heap.

The different growing directions of stack & heap requires the MMU to
add one bit to each segment so as to keep track of in thich direction
it grows.

Adding hardware *protection bits* the control whether a process can
read, write, or execute a particular segment enables *code sharing*,
where one read-only (and executable) segment can be shared by multiple
processes running the same program.

However, as processes are created, the available physical memory space
can still be divided into small chunks, a problem called *external
fragmentation*.  [[https://stackoverflow.com/a/24980816/11938767][Here]]'s an excellent clarification of external vs
internal fragmentation.

Allocating variable-sized segments in memory leads to many problems.
The only solution is to never allocate memory in variable-sized
chunks.  Segmentation still isn't flexible enough.

* 17. Free-Space Management

Managing variable-sized memory units can be a challenging task.  Many
solutions are proposed that deal with external fragmentation.

** Mechanisms

There are three common mechanisms used in most allocators:
- splitting & coalescing (merging): splitting a free unit into two
  smaller chunks and merging two neighboring free units into a big
  one.
- tracking the size of allocated regions: =free(void *ptr)= does not
  take a size argument; thus it is assumed that given a pointer, the
  malloc library can quickly determine the size of the region of
  memory being freed and thus incorporate the space back into the free
  list.  Normally, a header structure (=header_t=) is placed just
  before the address returned by =malloc=.  So the total allocated
  space is user requested size plus size of the header structure.
  #+begin_src c
    typedef struct {
      int size;
      int magic;
    } header_t;
  #+end_src
- embedding a free list: Managing the free list requires building a
  free list inside the free space itself.  Another header structure
  (=node_t=) is needed for each free space.  It is also immediately
  before the free space, denoting its size and the next free node in
  the list.
  #+begin_src c
    typedef struct __node_t {
      int              size;
      struct __node_t *next;
    } node_t;
  #+end_src
  =malloc= returns the address of the allocated space *after*
  =header_t=, while the =next= field of =node_t= points to the
  =node_t= of the next free portion.  The address of =node_t= starts a
  documented free portion, not the address after it.
- growing the heap

** Basic Strategies

When a request for allocating a particular size of memory arrives,
there are a few basic strategies:
- best fit: traverse the free list, return the smallest region
  satisfying the request.  This can result in many very small
  fragmentations.  Also, traversing the free list is a very expensive
  operation.
- worst fit: return the largest region satisfying the request.  This
  approach tries to minimize the very small fragmentations produced by
  /best fit/.  However, this performs badly, leading to excess
  fragmentation while still having high overheads.
- first fit: simply return the first blog that is big enough.
- next fit: instead of searching starting from the head of the list
  like /first fit/, /next fit/ remembers the entry last allocated, and
  starts searching after that.  The idea is to spread the searches
  throughout the list more uniformly.

** Other Approaches

*** Segregated Lists

Segregated lists tries to *make the common case fast* by observing
whether a particular application has one (or a few) *popular-sized*
request that it makes, and maintaining a separate list just to manage
objects of that size.

*** Buddy Allocation

Binary buddy allocator tries to ease the merging of two free spaces.
The size of total heap is $2^N$.  The size of a free space within the
heap is always $2^n$ ($n < N$), forming a tree-like structure.  During
an malloc request, the allocator starts with $2^N$, and recurses down
the tree to the smallest chuck of size $2^m$.  When that portion is
freed, the allocator checks whether its sibling (aka. its buddy) is
also free, if it is, then merges the two units.  Note that this
approach suffers from internal fragmentation, as you are only allowed
to give out power-of-two-sized blocks.

* 18. Paging: Introduction

Instead of splitting up a process's address space into some number of
variable-sized logical segments (e.g., code, heap, stack), we divide
it into *fixed-sized units*, each of which we call a *page*.  The
physical memory is viewed as an array of fixed-sized slots called
*page frames*; each of the frames can contain a single virtual-memory
page.  Correspondingly, we view physical memory as an array of
fixed-sized slots called *page frames*; each of these frames can
contain a single virtual-memory page.

The OS keeps a page table for *each* running process, since each one
has a separate address space.  In a page table, each *pate table entry
(PTE)* stores a mapping between a *virtual page number (VPN)* to a
*physical frame number (PFN)*.  On a memory access, the virtual
address is split into a VPN and an offset.  The OS first consults the
*page table entry* (indexed by the VPN) for the PFN of the requested
address, then concatenates PFN with the offset, producing the actual
physical address.

The raw form of paging has two problems:
- too large: page table can be very large: on a 32-bit physical memory
  (4GB) with a 4KB page table, a single page table can have as many as
  $2^{32} / 2^{12} = 2^{20}$ PTEs.  If each PTE takes up 4B, then that
  would be a total of 4MB per process just to store its page table.
  If there are 100 processes, then 400MB will be needed!
- too slow: each memory access needs to consult the page table, which
  is another memory access overhead, making the process slower by at
  least a factor of two

* 19. Paging: Faster Translations (TLBs)

A translation-lookaside buffer (TLB) is part of the chip's
memory-management unit (MMU), and is simply a *hardware cache* of
popular virtual-to-physical address translations; thus, a better name
would be an address-translation cache.

On each memory access, the hardware first checks if the TLB holds the
entry for the VPN of the virtual address.  Just like a regular cache
has hits & misses, there can be a TLB hit or a TLB miss.  On a TLB
hit, the TLB simply calculates the physical address and accesses the
memory.  On a TLB miss, however, the TLB will have to consult the
original (full) page table in memory for the PTE, store it in TLB, and
retry the access operation.

** Who Handles the TLB Miss?

*** Hardware

In the old days of CISC computers, hardware handles TLB misses
entirely.  To do this, the hardware needs to know everything about the
page table: where they are stored in memory and the exact data
structure.

*** Software

On RISC machines, the hardware simply raises an exception on a TLB
miss and let the OS handles it.  When the OS is done updating the TLB,
the hardware retries the access.

This raises a few issues:
- different from the return-from-trap instruction in other exception
  handlers that resumes execution *after* the instruction that raised
  the exception, the return-from-trap in a TLB miss needs to resume
  execution *still at* the memory access instruction that caused the
  miss, not after it, so the memory access is retried.
- when handling a TLB miss, the OS needs to be careful not to raise an
  infinite chain of TLB misses to occur.

** On Context Switch

Since each process has a different page table, the TLB entries for the
last process are meaningless to the about-to-be-run process.  Some
hardware systems provide an address space identifier (ASID) field in
each TLB entry to distinguish between the address spaces of different
processes.  An ASID is just like a process identifier (PID) except it
has fewer bits.  With ASIDs, the TLB can hold translations from
different processes at the same time without confusion.

* 20. Paging: Smaller Tables

Simple array-based page tables (aka. linear page tables) are too big.
Several solutions are proposed in order to make page tables smaller.
Using bigger page sizes can cause internal fragmentation; as for the
hybrid approach of having one page table *per segment* (e.g. code,
heap, stack) for each process, it now introduces variable-sized page
tables, which can be hard to manage and will again cause external
fragmentation to arise.

** Multi-level Page Tables

This approach does not use segmentation, instead it turns the page
table into a tree-like structure.  The page table is divided into
page-sized units, each one being.  An extra *page directory* is used
to track where each page-sized unit is in memory (or whether the unit
is not allocated because all the PTEs therein are not valid).

The page directory, in a simple two-level table, consists of a number
of *page directory entries* (PDE).  A PDE (minimally) has a *valid
bit* and a *page frame number* (PFN), similar to a PTE.  If the PDE is
valid, it means that *at least one* of the pages of the page table
that the entry points to (via the PFN) is valid, i.e., in at least one
PTE on that page pointed to by this PDE, the valid bit in that PTE is
set to one.  If the PDE is not valid (i.e., equal to zero), the rest
of the PDE is not defined.

The multi-level page table adds a *level of indirection* through the
use of page directory, allowing page table units to reside on
arbitrary location in memory, as opposed to the linear page table that
requires all PTEs reside continuously in physical memory.

The goal of constructing a multi-level page table is to make each
piece of the page table (including the page table directory) fit
within a single page.  However, in a two-level page table, if there
are too many page directory entries, the page table directory may not
fit within a single page.  This requires the page table to have more
than two levels.
